{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je n'ai pas assez d'informations pour répondre à cette question.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI, ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ChatMessageHistory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Utilisation du modèle d'embeddings GoogleGenerativeAI\n",
    "def chunk_embedder():\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", temperature=0.5)\n",
    "    return embeddings\n",
    "\n",
    "def load_llm():\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "    return llm\n",
    "\n",
    "def init_chat():\n",
    "    llm = load_llm()\n",
    "    embeddings = chunk_embedder()\n",
    "\n",
    "    # Connexion à Qdrant\n",
    "     # Connexion à Qdrant\n",
    "    client = QdrantClient(url=\"http://localhost:6333\")  # Utiliser le bon endpoint\n",
    "    collection_name = \"BFT_chatbot\"\n",
    "\n",
    "\n",
    "    # Vérifier si la collection existe déjà\n",
    "    if not client.get_collections().collections or collection_name not in [c.name for c in client.get_collections().collections]:\n",
    "        # Créer la collection si elle n'existe pas\n",
    "        client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=768, distance=Distance.COSINE)  # 768 car c'est la dimension de ton modèle\n",
    "        )\n",
    "        # Charger et ajouter des documents initiaux si la collection n'existe pas encore\n",
    "        loader = CSVLoader(\"C:/Users/Nflak/OneDrive/Desktop/bft_chatbot/datas/faq_clcam.csv\", encoding=\"utf-8\")\n",
    "        content = loader.load()\n",
    "\n",
    "        # Diviser les documents en chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=3)\n",
    "        docs = text_splitter.split_documents(documents=content)\n",
    "\n",
    "        # Initialiser et ajouter les documents au vectorstore\n",
    "        vectorstore = Qdrant(\n",
    "            client=client, \n",
    "            collection_name=collection_name, \n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        vectorstore.add_documents(docs)\n",
    "    else:\n",
    "        # Charger l'existant si la collection est déjà là\n",
    "        vectorstore = Qdrant(\n",
    "            client=client, \n",
    "            collection_name=collection_name, \n",
    "            embeddings=embeddings\n",
    "        )\n",
    "\n",
    "    # Modèle de prompt pour l'assistant\n",
    "    template = \"\"\"\n",
    "    Tu es un assistant virtuel de BFT formé pour répondre aux questions fréquemment posées par les agents de terrain, \n",
    "    les caissiers, et les gestionnaires d'agences des institutions de microfinance. Ton objectif est de fournir des réponses \n",
    "    précises et claires pour résoudre leurs problèmes rapidement. Les questions peuvent concerner des procédures internes, \n",
    "    des problèmes techniques ou des informations sur les services de BFT. Utilise aussi les informations de ce site \"https://bftgroup.co/\"\n",
    "    qui est le site officiel de BFT. \n",
    "    Si une question a plusieurs réponses ou options, donne toutes les réponses sans exception. \n",
    "    Si la question posée se trouve dans la FAQ, copie et renvoie exactement la réponse de la FAQ sans reformulation ni modification. \n",
    "    En plus des informations fournies dans le document, utilise tes capacités en tant que Gemini Pro pour enrichir la réponse, \n",
    "    mais assure-toi de toujours fournir les informations les plus cohérentes et pertinentes. Si tu ne trouves pas suffisamment \n",
    "    d'informations, réponds par : \"Je n'ai pas assez d'informations pour répondre à cette question.\" Cependant, pour toutes les \n",
    "    questions basiques ou liées à toi (comme ta fonction, ton utilité, ou des questions de politesse), réponds de manière adaptée.\n",
    "    context: {context}\n",
    "    Question : {question}\n",
    "    output\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Mémoire de conversation pour l'assistant (persistance)\n",
    "    message_history = ChatMessageHistory()\n",
    "    chat_memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        chat_memory=message_history,\n",
    "        k=5,\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "    # Définir le retriever pour le vectorstore\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "    \n",
    "    # Chaîne de conversation pour la récupération d'informations et LLM\n",
    "    conversation = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=chat_memory,\n",
    "        combine_docs_chain_kwargs=chain_type_kwargs,\n",
    "    )\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "conversation = init_chat()\n",
    "\n",
    "# Fonction pour interagir avec le chatbot\n",
    "def Talker(query: str, conversation):\n",
    "    output = conversation.invoke(input=query)\n",
    "    response = output.get('answer')\n",
    "    return response\n",
    "\n",
    "# Exemple d'appel de la fonction Talker\n",
    "print(Talker(\" Que faire lorsque après enregistrement, le fichier d'enregistrement du client ne vient pas sur la plateforme Smart Saving?\", conversation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
